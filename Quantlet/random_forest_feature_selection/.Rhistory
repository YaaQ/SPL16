library(dplyr)
rm(list = ls())
## setting up basic matrix using base matrix type
rho = -0.2
sig_e = 0.3
FF = matrix(c(rho, 1, 0, 0, 0, 1, 0, 0, 0), ncol = 3)
In = diag(9)
QQ = matrix(c(sig_e^2, 0, 0, 0, 0, 0, 0, 0, 0), ncol = 3)
vecQQ = matrix(QQ, ncol = 1)
#as(QQ, 'TsparseMatrix')
#vecQQ = Matrix(c(sig_e^2, 0, 0, 0, 0, 0, 0, 0, 0), ncol = 1)
#as(QQ, 'sparseVector')
#Matrix(c(sig_e^2, 0, 0, 0, 0, 0, 0, 0, 0), ncol = 1)
PP = solve(In - kronecker(FF, FF), vecQQ)
PP1 = forwardsolve(In - kronecker(FF, FF), vecQQ)
all.equal(PP, PP1)
identical(PP, PP1)
## do this to multi individuals using
nn = 30
I3 = matrix(0, ncol = 3, nrow = 3); I3[1, 1] = 1
Inn = diag(nn)
bigF = kronecker(Inn, FF)
aa = nrow(bigF)
bigIn = diag(aa^2)
bigQ = diag(x = rnorm(nn))
bigQ = kronecker(bigQ, I3)
vecbigQ = matrix(bigQ, ncol = 1)
# PP = solve(bigIn - kronecker(bigF, bigF), vecbigQ)  # too slow
PP1 = forwardsolve(bigIn - kronecker(bigF, bigF), vecbigQ) # fast
#identical(PP, PP1)
#all.equal(PP, PP1)
## for sparse matrix
spbigIn  = Diagonal(n = aa^2)
spInn = Diagonal(n = nn)
spbigF = kronecker(spInn, FF)
spvbigQ = Matrix(vecbigQ)
PP = solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ)
all.equal(PP1, as(PP, 'matrix'))
identical(PP1, as(PP, 'matrix'))
nn = 30
I3 = matrix(0, ncol = 3, nrow = 3); I3[1, 1] = 1
Inn = diag(nn)
bigF = kronecker(Inn, FF)
aa = nrow(bigF)
bigIn = diag(aa^2)
bigQ = diag(x = rnorm(nn))
bigQ = kronecker(bigQ, I3)
vecbigQ = matrix(bigQ, ncol = 1)
# PP = solve(bigIn - kronecker(bigF, bigF), vecbigQ)  # too slow
PP1 = forwardsolve(bigIn - kronecker(bigF, bigF), vecbigQ) # fast
spbigIn  = Diagonal(n = aa^2)
spInn = Diagonal(n = nn)
spbigF = kronecker(spInn, FF)
spvbigQ = Matrix(vecbigQ)
PP = solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ)
all.equal(PP1, as(PP, 'matrix'))
identical(PP1, as(PP, 'matrix'))
spbigIn  = Diagonal(n = aa^2)
spInn = Diagonal(n = nn)
spbigF = kronecker(spInn, FF)
spvbigQ = Matrix(vecbigQ)
PP = solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ)
all.equal(PP1, as(PP, 'matrix'))
identical(PP1, as(PP, 'matrix'))
PP = solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ)
PP2 = solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ)
PP = solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ)
PP2 = solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ)
?microbenchmark
??microbenchmark
all.equal(PP, PP2)
identical(PP, PP2)
head(PP)
head(PP1)
str(PP)
str(PP1)
str(PP2)
?repeat
?lapply
compare = function(f) {
system.time(replicate(n = 100, expr = f))
}
compare(solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ))
compare(solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ), spvbigQ))
compare(solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ), spvbigQ)
compare(solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ))
compare(solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ))
compare(solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ))
compare = function(f) {
system.time(replicate(n = 1000, expr = f))
}
compare(solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ))
compare(solve(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'), spvbigQ))
replicate(1000, solve(spbigIn - kronecker(spbigF, spbigF), spvbigQ))
compare = function(x) {
system.time(replicate(n = 1000, expr = solve(x, spvbigQ)))
}
compare(spbigIn - kronecker(spbigF, spbigF))
compare(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'))
compare(spbigIn - kronecker(spbigF, spbigF))
compare(as(spbigIn - kronecker(spbigF, spbigF), 'triangularMatrix'))
aa = matrix(rnorm(2500), ncol = 50)
AA = Matrix(aa)
system.time(replicate(100, aa %*% aa))
system.time(replicate(100, function(x = aa) x %*% x))
system.time(replicate(1000, function(x = aa) x %*% x))
system.time(aa %*% aa)
rr = 500
aa = matrix(rnorm(rr^2), ncol = rr)
AA = Matrix(aa)
system.time(replicate(100, function(x = aa) x %*% x))
system.time(aa %*% aa)
system.time(AA %*% AA)
system.time(aa %*% aa)
system.time(AA %*% AA)
system.time(aa %*% aa)
system.time(AA %*% AA)
rr = 500
aa = matrix(rnorm(rr^2), ncol = rr)
AA = Matrix(aa)
system.time(aa %*% aa)
system.time(AA %*% AA)
rr = 500
aa = matrix(rnorm(rr^2), ncol = rr)
AA = Matrix(aa)
system.time(aa %*% aa)
system.time(AA %*% AA)
system.time(AA %*% aa)
system.time(aa %*% aa)
system.time(AA %*% aa)
system.time(aa %*% aa)
system.time(AA %*% aa)
library(h2o)
source("load_ames_data.R")
source("utils/quick_preprocessing.R")
source("utils/performanceMetrics.R")  # to get performance metrics
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# Next, we download packages that H2O depends on.
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("jsonlite" %in% rownames(installed.packages()))) { install.packages("jsonlite") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }
# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/rel-turnbull/1/R")))
library(h2o)
localH2O = h2o.init(nthreads=-1)
h2o.clusterInfo(localH2O)
library(h2o)
source("load_ames_data.R")
source("utils/quick_preprocessing.R")
library(h2o)
source("load_ames_data.R")
setwd('F:/PHD/IRTG/courses/SPL')
source("load_ames_data.R")
source("utils/quick_preprocessing.R")
source("utils/performanceMetrics.R")  # to get performance metrics
h2o.init(
nthreads=-1,            ## -1: use all available threads (use all cores)
max_mem_size = "2G")    ## specify the memory size for the H2O cloud
h2o.removeAll() # Clean slate - just in case the cluster was already running
train <- basic_preprocessing(X_com,y)$train
train_h2o <- as.h2o(train)
# sepecify columns of inputs and labels
col_label <- which(colnames(train) == "y")
col_input <- which(colnames(train) != "y")
rfFit <- h2o.randomForest(           # h2o.randomForest function
training_frame = train_h2o,        # H2O frame for training
x=col_input,                       # predictor columns, by column index
y=col_label,                       # label column index
model_id = "rf_covType_v1",        # name the model in H2O
ntrees = 500,                      # number of trees
mtries = 20,                       # number of variable considered at each split
sample_rate = 0.8,                 # fraction of booststrap sample
max_depth = 25
)
h2o.varimp_plot(rfFit)
rfFit@model$variable_importances$percentage
plot(rfFit@model$variable_importances$percentage)
library(ggplot2)
ranked_variables <- rfFit@model$variable_importances$variable
importance_per <- rfFit@model$variable_importances$percentage
var_imp <- data.frame(ranked_variables,importance_per)[1:30,]
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip()
print(p)
library(ggplot2)
ranked_variables <- rfFit@model$variable_importances$variable
importance_per <- rfFit@model$variable_importances$percentage
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip()
print(p)
retained_variables <- 1:nrow(var_imp)
variance_level <- cumsum(importance_per)
retained <- data.frame(variance_level,retained_variables)
p <- ggplot(data = retained, mapping = aes(retained_variables,variance_level))
p <- p + geom_line() + geom_point()
p <- p + ggtitle("Retained Variables vs. Cumsum VI") + xlab("# Variables") + ylab("Cumsum of VI")
print(p)
retained_variables <- 1:nrow(var_imp)
variance_level <- cumsum(importance_per)
retained <- data.frame(variance_level,retained_variables)
length(variance_level)
length(retained_variables)
retained_variables <- 1:nrow(var_imp)
variance_level <- cumsum(importance_per)
retained <- data.frame(variance_level,retained_variables)
p <- ggplot(data = retained, mapping = aes(retained_variables,variance_level))
p <- p + geom_line() + geom_point()
p <- p + ggtitle("Retained Variables vs. Cumsum VI") + xlab("# Variables") + ylab("Cumsum of VI")
print(p)
setwd('Quantlet/data_processing/')
rm(list = ls())
source('quick_preprocessing.R')
train <- read.csv("Data/ames_train.csv", header=T)
test <- read.csv("Data/ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
source('quick_preprocessing.R')
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
train <- read.csv("Data/ames_train.csv", header=T)
test <- read.csv("Data/ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
train <- read.csv("ames_train.csv", header=T)
test <- read.csv("ames_test.csv", header=T)
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
processed_data = basic_preprocessing(X_com, y)
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
processed_data = basic_preprocessing(X_com, y)
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
train <- read.csv("ames_train.csv", header=T)
test <- read.csv("ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
processed_data = basic_preprocessing(X_com, y)
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
train <- read.csv("ames_train.csv", header=T)
test <- read.csv("ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
processed_data = basic_preprocessing(X_com, y)
str(processed_data)
save.image("F:/PHD/IRTG/courses/SPL/Quantlet/data_processing/basic_processing.RData")
setwd('F:/PHD/IRTG/courses/SPL/Quantlet/random_forest')
source('basic_processing.RData')
rm(list = ls())
source('basic_processing.RData')
load('basic_processing.RData')
setwd('F:/PHD/IRTG/courses/SPL/Quantlet/data_processing')
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
train <- read.csv("ames_train.csv", header=T)
test <- read.csv("ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
basic_data = basic_preprocessing(X_com, y)
### save workplace for further use
save.image("F:/PHD/IRTG/courses/SPL/Quantlet/data_processing/basic_processing.RData")
setwd('F:/PHD/IRTG/courses/SPL/Quantlet/random_forest')
rm(list = ls())
### install the package h2o
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# Next, we download packages that H2O depends on.
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("jsonlite" %in% rownames(installed.packages()))) { install.packages("jsonlite") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }
# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/rel-turnbull/1/R")))
library(h2o)
localH2O = h2o.init(nthreads=-1)
h2o.clusterInfo(localH2O)
load('basic_processing.RData')
train <- basic_data$train
setwd('F:/PHD/IRTG/courses/SPL/Quantlet/data_processing')
basic_preprocessing <- function(X_com,y, scaler = "gaussian"){
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings <- replace_ratings(X_com)
X_imputed <- naive_imputation(X_ratings)
X_no_outlier <- data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded <- include_quarter_dummies(X_no_outlier)
X_scaled <- scale_data(X_time_encoded, scale_method = scaler)
X_encoded <- data.frame(lapply(X_scaled, cat_to_dummy))
X_com <- delect_nz_variable(X_encoded)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_com[idx_train,],y)
test <- X_com[-idx_train,]
return(list(train=train,X_com=X_com, test=test)) # return without id
}
train <- read.csv("ames_train.csv", header=T)
test <- read.csv("ames_test.csv", header=T)
# split target variable and feature matrix
y <- train[,81] # target variable SalePrice
X <- train[,-81] # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com <- rbind(X,test)
basic_data = basic_preprocessing(X_com, y)
### save workplace for further use
save.image("F:/PHD/IRTG/courses/SPL/Quantlet/data_processing/basic_processing.RData")
setwd('F:/PHD/IRTG/courses/SPL/Quantlet/random_forest')
rm(list = ls())
library(h2o)
load('basic_processing.RData')
train <- basic_data$train
### model preparing
localH2O = h2o.init(nthreads=-1)
h2o.clusterInfo(localH2O)
train_h2o <- as.h2o(train)
# sepecify columns of inputs and labels
col_label <- which(colnames(train) == "y")
col_input <- which(colnames(train) != "y")
rfFit <- h2o.randomForest(           # h2o.randomForest function
training_frame = train_h2o,        # H2O frame for training
x=col_input,                       # predictor columns, by column index
y=col_label,                       # label column index
model_id = "rf_covType_v1",        # name the model in H2O
ntrees = 500,                      # number of trees
mtries = 20,                       # number of variable considered at each split
sample_rate = 0.8,                 # fraction of booststrap sample
max_depth = 25
)
### output results using figures
h2o.varimp_plot(rfFit)
ranked_variables <- rfFit@model$variable_importances$variable
importance_per <- rfFit@model$variable_importances$percentage
var_imp <- data.frame(ranked_variables,importance_per)[1:30,]
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip()
print(p)
ranked_variables <- rfFit@model$variable_importances$variable
importance_per <- rfFit@model$variable_importances$percentage
var_imp <- data.frame(ranked_variables,importance_per)[1:30,]
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip() + xlab('')
print(p)
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip() + xlab('')
print(p)
?graphics.off
length(ranked_variables)
length(importance_per)
var_imp <- data.frame(ranked_variables,importance_per)[1:20,]
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip() + xlab('')
print(p)
p <- ggplot(var_imp, aes(x=reorder(ranked_variables, importance_per), weight=importance_per, fill=ranked_variables))
p <- p + geom_bar(aes(weights=importance_per)) +
ggtitle("Variable Importance from Random Forest Fit") + theme(legend.position="none") + coord_flip() + xlab('')
p
