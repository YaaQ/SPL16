X_scaled = scale_data(X_time_encoded, scale_method = scaler)
X_encoded = data.frame(lapply(X_scaled, cat_to_dummy))
X_com = delect_nz_variable(X_encoded)
# remerge train data
idx_train = c(1:length(y))
train = cbind(X_com[idx_train, ], y)
test = X_com[-idx_train, ]
return(list(train = train, X_com = X_com, test = test))  # return without id
}
train = read.csv("ames_train.csv", header = T)
setwd("~/SPL16/Quantlet/data_processing")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/data_processing')
rm(list = ls())
graphics.off()
basic_preprocessing = function(X_com, y, scaler = "gaussian") {
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings = replace_ratings(X_com)
X_imputed = naive_imputation(X_ratings)
X_no_outlier = data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded = include_quarter_dummies(X_no_outlier)
X_scaled = scale_data(X_time_encoded, scale_method = scaler)
X_encoded = data.frame(lapply(X_scaled, cat_to_dummy))
X_com = delect_nz_variable(X_encoded)
# remerge train data
idx_train = c(1:length(y))
train = cbind(X_com[idx_train, ], y)
test = X_com[-idx_train, ]
return(list(train = train, X_com = X_com, test = test))  # return without id
}
train = read.csv("ames_train.csv", header = T)
test = read.csv("ames_test.csv", header = T)
# split target variable and feature matrix
y = train[, 81]  # target variable SalePrice
X = train[, -81]  # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com = rbind(X, test)
basic_data = basic_preprocessing(X_com, y)
View(X_com)
basic_preprocessing = function(X_com, y, scaler = "gaussian") {
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings = replace_ratings(X_com)
X_imputed = naive_imputation(X_ratings)
X_no_outlier = data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded = include_quarter_dummies(X_no_outlier)
X_scaled = scale_data(X_time_encoded, scale_method = scaler)
X_encoded = data.frame(lapply(X_scaled, cat_to_dummy))
X_com = delect_nz_variable(X_encoded)
# remerge train data
idx_train = c(1:length(y))
train = cbind(X_com[idx_train, ], y)
test = X_com[-idx_train, ]
return(list(train = train, X_com = X_com, test = test))  # return without id
}
train = read.csv("ames_train.csv", header = T)
test = read.csv("ames_test.csv", header = T)
basic_data = data.frame(basic_preprocessing(X_com, y))
X_com <- basic_data$X_com
train <- basic_data$train
test <- basic_data$test
load("C:/Users/dkohn/AppData/Local/Temp/basic_processing.RData")
data <- load("C:/Users/dkohn/AppData/Local/Temp/basic_processing.RData")
result_path <- "Quantlet/data_preprocessing/"
paste(result_path,"inputMatrix.csv", sep="_")
write.csv(X_com, paste(result_path,"inputMatrix.csv", sep="_"))
result_path <- "Quantlet/data_preprocessing/"
paste(result_path,"inputMatrix.csv", sep="")
result_path <- "SPL/Quantlet/data_preprocessing/"
write.csv(X_com, paste(result_path,"inputMatrix.csv", sep=""))
paste(result_path,"inputMatrix.csv", sep="")
result_path <- "SPL16/Quantlet/data_preprocessing/"
write.csv(X_com, paste(result_path,"inputMatrix.csv", sep=""))
write.csv(X_com, paste(result_path,"inputMatrix.csv", sep=""))
write.csv(X_com, "inputMatrix.csv")
write.csv(train,"train_preprocessed.csv")
write.csv(X_com, "inputMatrix_preprocessed.csv")
write.csv(train,"test_preprocessed.csv")
rm(list = ls())
graphics.off()
basic_preprocessing = function(X_com, y, scaler = "gaussian") {
source("replace_ratings.R")
source("convert_categoricals.R")
source("impute_data.R")
source("encode_time_variables.R")
source("impute_outliers.R")
source("scale_data.R")
source("delete_nearzero_variables.R")
X_ratings = replace_ratings(X_com)
X_imputed = naive_imputation(X_ratings)
X_no_outlier = data.frame(lapply(X_imputed, iqr_outlier))
X_time_encoded = include_quarter_dummies(X_no_outlier)
X_scaled = scale_data(X_time_encoded, scale_method = scaler)
X_encoded = data.frame(lapply(X_scaled, cat_to_dummy))
X_com = delect_nz_variable(X_encoded)
# remerge train data
idx_train = c(1:length(y))
train = cbind(X_com[idx_train, ], y)
test = X_com[-idx_train, ]
return(list(train = train, X_com = X_com, test = test))  # return without id
}
train = read.csv("ames_train.csv", header = T)
test = read.csv("ames_test.csv", header = T)
y = train[,"SalePrice"]  # target variable SalePrice
# merge test and train features to get the complete feature matrix
X_com = rbind(X, test)
# apply preprocessing
basic_data = basic_preprocessing(X_com, y)
# get the complete input matrix
X_com <- basic_data$X_com
write.csv(X_com, "inputMatrix_preprocessed.csv")
# get the training data with labels
train <- basic_data$train
write.csv(train,"train_preprocessed.csv")
# get the test data to be predicted in the same format
test <- basic_data$test
write.csv(train,"test_preprocessed.csv")
X_com = rbind(X, test)
# split target variable and feature matrix
y = train[, "SalePrice"]  # target variable SalePrice
X = train[, -81]  # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com = rbind(X, test)
# apply preprocessing
basic_data = basic_preprocessing(X_com, y)
# get the complete input matrix
X_com <- basic_data$X_com
write.csv(X_com, "inputMatrix_preprocessed.csv")
# get the training data with labels
train <- basic_data$train
write.csv(train,"train_preprocessed.csv")
# get the test data to be predicted in the same format
test <- basic_data$test
write.csv(train,"test_preprocessed.csv")
setwd("~/SPL16/Quantlet/data_processing")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/data_processing')
rm(list = ls())
graphics.off()
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_preprocessing.R")
source("pca_basic.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
# Function to perform preprocessing steps + pca
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_preprocessing.R")
source("pca_basic.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
train = read.csv("ames_train.csv", header = T)
test = read.csv("ames_test.csv", header = T)
# split target variable and feature matrix
y = train[, "SalePrice"]  # target variable SalePrice
X = train[, -81]  # feature matrix without target variable
# merge test and train features to get the complete feature matrix
X_com = rbind(X, test)
# apply preprocessing
basic_data = pca_preprocessing(X_com, y)
# get the complete input matrix
pca_data = pca_preprocessing(X_com, y, 0.8)
# Function to perform preprocessing steps + pca
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_processing.R")
source("pca_basic.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
pca_data = pca_preprocessing(X_com, y, 0.8)
pca_data = pca_preprocessing(X_com, y, 0.8)
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_processing.R")
source("pca_basic.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
pca_data = pca_preprocessing(X_com, y, 0.8)
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_processing.R")
source("pca.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
# load raw d
pca_data = pca_preprocessing(X_com, y, 0.8)
source("basic_processing.R")
basic_processing
# Function to perform preprocessing steps + pca
pca_preprocessing <- function(X_com, y, contained_variance){
source("basic_processing.R")
source("pca.R")
X_basic <- basic_preprocessing(X_com,y)$X_com
X_pca <- pca(X_basic, contained_variance)
# remerge train data
idx_train <- c(1:length(y))
train <- cbind(X_pca[idx_train,],y)
test <- X_pca[-idx_train,]
return(list(train=train,X_com=X_pca, test=test)) # return without id
}
source("basic_processing.R")
source("basic_processing.R")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/random_forest_feature_selection')
rm(list = ls())
graphics.off()
### load processed data
load("basic_processing.RData")
train = basic_data$train
### model preparing
localH2O = h2o.init(nthreads = -1)
h2o.clusterInfo(localH2O)
train_h2o = as.h2o(train)
# sepecify columns of inputs and labels
col_label = which(colnames(train) == "y")
col_input = which(colnames(train) != "y")
### fit the Random Forest
rfFit = h2o.randomForest(training_frame = train_h2o, x = col_input, y = col_label, model_id = "rf_covType_v1",
ntrees = 500, mtries = 20, sample_rate = 0.8, max_depth = 25)
### model preparing
localH2O = h2o.init(nthreads = -1)
h2o.clusterInfo(localH2O)
train_h2o = as.h2o(train)
# sepecify columns of inputs and labels
col_label = which(colnames(train) == "y")
col_input = which(colnames(train) != "y")
libraries = c("ggplot2")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(c(libraries, "h2o"), library, quietly = TRUE, character.only = TRUE)
localH2O = h2o.init(nthreads = -1)
train_h2o = as.h2o(train)
# sepecify columns of inputs and labels
col_label = which(colnames(train) == "y")
col_input = which(colnames(train) != "y")
### fit the Random Forest
rfFit = h2o.randomForest(training_frame = train_h2o, x = col_input, y = col_label, model_id = "rf_covType_v1",
ntrees = 500, mtries = 20, sample_rate = 0.8, max_depth = 25)
### output results using figures
h2o.varimp_plot(rfFit)
h2o.shutdown()
ranked_variables = rfFit@model$variable_importances$variable
importance_per = rfFit@model$variable_importances$percentage
var_imp = data.frame(ranked_variables, importance_per)[1:20, ]
p = ggplot(var_imp, aes(x = reorder(ranked_variables, importance_per), weight = importance_per,
fill = ranked_variables))
p = p + geom_bar(aes(weights = importance_per)) + ggtitle("Variable Importance from Random Forest Fit") +
theme(legend.position = "none") + coord_flip() + xlab("")
print(p)
setwd("~/SPL16/Quantlet/random_forest_turning")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/random_forest_turning')
rm(list = ls())
graphics.off()
######### if package 'h2o' is not installed before, run the following code ############
######### source('install_h2o.R')
library(h2o)
load("basic_processing.RData")
h2o.init(nthreads = -1, max_mem_size = "2G")
h2o.removeAll()  # Clean slate - just in case the cluster was already running
# get preprocessed training set
train = basic_data$train
test = basic_data$test
# specify repetions and number of folds
repetitions = 2
k_folds = 5
# convert into h20_objects
train_h2o = as.h2o(train)
# sepecify columns of inputs and labels
col_label = which(colnames(train) == "y")
col_input = which(colnames(train) != "y")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/svm_tuning')
rm(list = ls())
graphics.off()
libraries = c("caret")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
load("basic_processing.RData")
# get preprocessed data
train = basic_data$train
y = train$y
train$y = NULL
# set cv parameter
t = 10  # repetition on inner loop (here caret does it)
k = 5  # folds on the inner cv loop
setwd("~/SPL16/Quantlet/svm_tuning")
# set cv parameter
t = 1  # repetition on inner loop (here caret does it)
k = 5  # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1))
svmGaussianGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1), sigma = c(0.001, 0.01, 0.1, 1))
# determine evaluation method of the inner cv loop
ctrl = trainControl(method = "repeatedcv", number = t, repeats = k, verboseIter = FALSE)
# SVM with linear kernel
svmLinearFit = train(x = train, y = y, method = "svmLinear", trControl = ctrl, tuneGrid = svmLinearGrid,
metric = "RMSE", maximize = FALSE)
plot(svmLinearFit, main = "SVM fit with linear kernel")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/svm_tuning')
rm(list = ls())
graphics.off()
libraries = c("caret")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
load("basic_processing.RData")
# get preprocessed data
train = basic_data$train
y = train$y
train$y = NULL
# set cv parameter
t = 1  # repetition on inner loop (here caret does it)
k = 5  # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1))
svmGaussianGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1), sigma = c(0.001, 0.01, 0.1, 1))
ctrl = trainControl(method = "repeatedcv", number = t, repeats = k, verboseIter = FALSE)
svmLinearFit = train(x = train, y = y, method = "svmLinear", trControl = ctrl, tuneGrid = svmLinearGrid,
metric = "RMSE", maximize = FALSE)
svmGaussianFit = train(x = train, y = y, method = "svmRadial", trControl = ctrl, tuneGrid = svmGaussianGrid,
metric = "RMSE", maximize = FALSE)
# get preprocessed data
train = basic_data$train
y = train$y
#train$y = NULL
# set cv parameter
t = 1  # repetition on inner loop (here caret does it)
k = 5  # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1))
svmGaussianGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1), sigma = c(0.001, 0.01, 0.1, 1))
# determine evaluation method of the inner cv loop
ctrl = trainControl(method = "repeatedcv", number = t, repeats = k, verboseIter = FALSE)
# SVM with linear kernel
svmLinearFit = train(x = train, y = y, method = "svmLinear", trControl = ctrl, tuneGrid = svmLinearGrid,
metric = "RMSE", maximize = FALSE)
setwd("~/SPL16")
rm(list = ls())
graphics.off()
################################# Ridge Regression ######################################################################
# This script performs repeated cross-validation to tune SVM-Regression.
library(caret)
###########################Load Data Cleaning Scripts
source("load_ames_data.R")
source("utils/quick_preprocessing.R") # to perform the naive preprocessing step implemented in the beginning
source("utils/performanceMetrics.R")  #
# get preprocessed data
train <- basic_preprocessing(X_com,y)$train
y <- train$y
train$y <- NULL
# set cv parameter
t <- 1 # repetition on inner loop (here caret does it)
k <- 5 # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid <-  expand.grid(C = c(0.001,0.01,0.1,1))
svmGaussianGrid <- expand.grid(C = c(0.001,0.01,0.1,1), sigma = c(0.001,0.01,0.1,1))
# determine evaluation method of the inner cv loop
ctrl <- trainControl(method="repeatedcv",
number=t,
repeats=k,
verboseIter=FALSE
)
# SVM with linear kernel
svmLinearFit <- train(x = train,
y = y,
method = 'svmLinear',  # method
trControl = ctrl,  # evaluatio method (repeated CV)
tuneGrid = svmLinearGrid, # grid
metric = "RMSE",  # error metric
maximize = FALSE
)
warning()
# set cv parameter
t <- 2  # repetition on inner loop (here caret does it)
k <- 5 # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid <-  expand.grid(C = c(0.001,0.01,0.1,1))
svmGaussianGrid <- expand.grid(C = c(0.001,0.01,0.1,1), sigma = c(0.001,0.01,0.1,1))
# determine evaluation method of the inner cv loop
ctrl <- trainControl(method="repeatedcv",
number=t,
repeats=k,
verboseIter=FALSE
)
# SVM with linear kernel
svmLinearFit <- train(x = train,
y = y,
method = 'svmLinear',  # method
trControl = ctrl,  # evaluatio method (repeated CV)
tuneGrid = svmLinearGrid, # grid
metric = "RMSE",  # error metric
maximize = FALSE
)
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/svm_tuning')
rm(list = ls())
graphics.off()
libraries = c("caret")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
load("basic_processing.RData")
# get preprocessed data
train = basic_data$train
y = train$y
train$y = NULL
setwd("~/SPL16/Quantlet/svm_tuning")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/svm_tuning')
rm(list = ls())
graphics.off()
libraries = c("caret")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
load("basic_processing.RData")
# get preprocessed data
train = basic_data$train
y = train$y
train$y = NULL
# set cv parameter
t = 2  # repetition on inner loop (here caret does it)
k = 5  # folds on the inner cv loop
# create Grid for GridSearch to tune hyperparameter
svmLinearGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1))
svmGaussianGrid = expand.grid(C = c(0.001, 0.01, 0.1, 1), sigma = c(0.001, 0.01, 0.1, 1))
# determine evaluation method of the inner cv loop
ctrl = trainControl(method = "repeatedcv", number = t, repeats = k, verboseIter = FALSE)
# SVM with linear kernel
svmLinearFit = train(x = train, y = y, method = "svmLinear", trControl = ctrl, tuneGrid = svmLinearGrid,
metric = "RMSE", maximize = FALSE)
ctrl = trainControl(method = "repeatedcv", number = t, repeats = k, verboseIter = TRUE)
# SVM with Gaussian Kernel
svmGaussianFit = train(x = train, y = y, method = "svmRadial", trControl = ctrl, tuneGrid = svmGaussianGrid,
metric = "RMSE", maximize = FALSE)
setwd("~/SPL16/Quantlet/xgboosting_feature_selection")
### setwd('F:/PHD/IRTG/courses/SPL/Quantlet/xgboosting_feature_selection')
rm(list = ls())
graphics.off()
libraries = c("caret", "xgboost", "Matrix", "ggplot2", "Ckmeans.1d.dp")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
# get preprocessed data
load("basic_processing.RData")
train = basic_data$train
y = train$y
train$y = NULL
# convert for data into a format xgb.train can handle
dtrain = xgb.DMatrix(data = sapply(train, as.numeric), label = y)
# determine arbitrary xgboost parameters in a list
xgb_paramters = list(eta = 0.025, max.depth = 16, gamma = 0, colsample_bytree = 0.8, subsample = 0.6,
eval_metric = "rmse", maximize = FALSE)
# fit the xgboost
xgbFit = xgb.train(params = xgb_paramters, data = dtrain, booster = "gbtree", nround = 500, verbose = 1,
objective = "reg:linear")
importance_matrix = xgb.importance(colnames(train), model = xgbFit)
### customized variable importance plot
ranked_variables = importance_matrix$Feature
importance_per = importance_matrix$Gain
var_imp = data.frame(ranked_variables, importance_per)[1:20, ]
p = ggplot(var_imp, aes(x = reorder(ranked_variables, importance_per), weight = importance_per,
fill = ranked_variables))
p = p + geom_bar(aes(weights = importance_per)) + ggtitle("Variable Importance from Gradient Boosting Fit") +
theme(legend.position = "none") + coord_flip() + xlab("")
print(p)
# plot variable importance
retained_variables = 1:nrow(importance_matrix)
variance_level = cumsum(importance_matrix$Gain)
retained = data.frame(variance_level, retained_variables)
p = ggplot(data = retained, mapping = aes(retained_variables, variance_level))
p = p + geom_line() + geom_point()
p = p + ggtitle("Retained Variables vs. Cumsum VI") + xlab("# Variables") + ylab("Cumsum of VI")
print(p)
